{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO演示 - 利用XGBoost模型预测客户流失\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background 业务背景介绍](#Background) \n",
    "1. [Setup 环境变量，依赖库配置](#Setup)\n",
    "1. [Data 数据预处理](#Data)\n",
    "1. [Train 模型训练](#Train)\n",
    "  - [Experiment 实验配置](#Experiment)\n",
    "  - [Debugger 调试配置](#Debugger)\n",
    "1. [Compile-Neo Neo编译](#Compile)\n",
    "1. [Host 模型推理托管](#Host)\n",
    "  - [Evaluate 模型评估](#Evaluate)\n",
    "  - [ModelMonitor 模型监控](#ModelMonitor)\n",
    "1. [Extensions 扩展](#Extensions)\n",
    "  - [AutoModelTuning 自动模型调优](#AutoModelTuning)\n",
    "  - [CleanUp 清理实验环境](#CleanUp)\n",
    "\n",
    "---\n",
    "\n",
    "## Background 背景介绍\n",
    "\n",
    "_This notebook has been adapted from an [AWS blog post](https://aws.amazon.com/blogs/ai/predicting-customer-churn-with-amazon-machine-learning/)_\n",
    "\n",
    "Losing customers is costly for any business.  Identifying unhappy customers early on gives you a chance to offer them incentives to stay.  This notebook describes using machine learning (ML) for the automated identification of unhappy customers, also known as customer churn prediction. ML models rarely give perfect predictions though, so this notebook is also about how to incorporate the relative costs of prediction mistakes when determining the financial outcome of using ML.\n",
    "\n",
    "We use an example of churn that is familiar to all of us–leaving a mobile phone operator.  Seems like I can always find fault with my provider du jour! And if my provider knows that I’m thinking of leaving, it can offer timely incentives–I can always use a phone upgrade or perhaps have a new feature activated–and I might just stick around. Incentives are often much more cost effective than losing and reacquiring a customer.\n",
    "\n",
    "<font color=#00BFFF size=3>失去客户对任何企业来说都是代价高昂的。 尽早识别出不满意的客户，可以使他们有机会留下来。激励措施通常比失去和重新获得客户更具成本效益。</font>\n",
    "\n",
    "---\n",
    "\n",
    "## Setup 环境设置\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "<font color=#00BFFF size=3></font>\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting. <font color=#00BFFF size=3>保证Notebook和S3在相同的区域</font>\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s). <font color=#00BFFF size=3>IAM角色允许Notebook实例访问S3上的数据</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "# 安装 实验 lib\n",
    "!{sys.executable} -m pip install -U sagemaker-experiments\n",
    "# 安装 debugger lib\n",
    "!{sys.executable} -m pip install -U smdebug\n",
    "# 重新启动内核\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker/demo-xgboost-churn'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the Python libraries we'll need for the remainder of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data 数据预处理\n",
    "\n",
    "Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes–after all, predicting the future is tricky business! But I’ll also show how to deal with prediction errors.\n",
    "\n",
    "<font color=#00BFFF size=3>基于移动终端客户的多种属性值的历史使用记录，训练模型，预测客户是否会流失</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "churn = pd.read_csv('./churn.txt')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By modern standards, it’s a relatively small dataset, with only 5,000 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n",
    "\n",
    "- `State`: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\n",
    "- `Account Length`: the number of days that this account has been active\n",
    "- `Area Code`: the three-digit area code of the corresponding customer’s phone number\n",
    "- `Phone`: the remaining seven-digit phone number\n",
    "- `Int’l Plan`: whether the customer has an international calling plan: yes/no\n",
    "- `VMail Plan`: whether the customer has a voice mail feature: yes/no\n",
    "- `VMail Message`: presumably the average number of voice mail messages per month\n",
    "- `Day Mins`: the total number of calling minutes used during the day\n",
    "- `Day Calls`: the total number of calls placed during the day\n",
    "- `Day Charge`: the billed cost of daytime calls\n",
    "- `Eve Mins, Eve Calls, Eve Charge`: the billed cost for calls placed during the evening\n",
    "- `Night Mins`, `Night Calls`, `Night Charge`: the billed cost for calls placed during nighttime\n",
    "- `Intl Mins`, `Intl Calls`, `Intl Charge`: the billed cost for international calls\n",
    "- `CustServ Calls`: the number of calls placed to Customer Service\n",
    "- `Churn?`: whether the customer left the service: true/false\n",
    "\n",
    "The last attribute, `Churn?`, is known as the target attribute–the attribute that we want the ML model to predict.  Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification.\n",
    "\n",
    "XGBoost 决策树模型，二分类和线性回归，本实验是二分类的方式。\n",
    "\n",
    "Let's begin exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature - 交叉表是用于统计分组频率的特殊透视表\n",
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=churn[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features - 每个特征的直方图\n",
    "# pandas有两个核心数据结构 Series和DataFrame，分别对应了一维的序列和二维的表结构。\n",
    "# 而describe()函数就是返回这两个核心数据结构的统计变量。其目的在于观察这一系列数据的范围、大小、波动趋势等等，为后面的模型选择打下基础。\n",
    "\n",
    "# 统计值变量说明\n",
    "# count：数量统计，此列共有多少有效值\n",
    "# std：标准差\n",
    "# min：最小值\n",
    "# 25%：四分之一分位数\n",
    "# 50%：二分之一分位数\n",
    "# 75%：四分之三分位数\n",
    "# max：最大值\n",
    "# mean：均值\n",
    "display(churn.describe())\n",
    "\n",
    "# 直方图\n",
    "%matplotlib inline\n",
    "hist = churn.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see immediately that:\n",
    "- `State` appears to be quite evenly distributed 分布比较均匀\n",
    "- `Phone` takes on too many unique values to be of any practical use.  It's possible parsing out the prefix could have some value, but without more context on how these are allocated, we should avoid using it. Phone基本都是一些唯一值，避免使用\n",
    "- Most of the numeric features are surprisingly nicely distributed, with many showing bell-like gaussianity.  `VMail Message` being a notable exception (and `Area Code` showing up as a feature we should convert to non-numeric). Area Code作为一项特征应当被转换为非数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop('Phone', axis=1)\n",
    "churn['Area Code'] = churn['Area Code'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at the relationship between each of the features and our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    if column != 'Churn?':\n",
    "        display(pd.crosstab(index=churn[column], columns=churn['Churn?'], normalize='columns'))\n",
    "\n",
    "for column in churn.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = churn[[column, 'Churn?']].hist(by='Churn?', bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(churn.corr())\n",
    "\n",
    "# scatter_matrix 绘制矩阵散点图\n",
    "pd.plotting.scatter_matrix(churn, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see several features that essentially have 100% correlation with one another.  Including these feature pairs in some machine learning algorithms can create catastrophic problems, while in others it will only introduce minor redundancy and bias.  Let's remove one feature from each of the highly correlated pairs: Day Charge from the pair with Day Mins, Night Charge from the pair with Night Mins, Intl Charge from the pair with Intl Mins:\n",
    "\n",
    "<font color=#00BFFF size=3>我们看到几个特征基本上具有100％的相关性。 在某些机器学习算法中包括这些功能对可能会造成灾难性的问题，而在其他机器学习算法中，只会引入较小的冗余和偏差。我们需要删除最支付相关的特征值 </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['Day Charge', 'Eve Charge', 'Night Charge', 'Intl Charge'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up our dataset, let's determine which algorithm to use.  As mentioned above, there appear to be some variables where both high and low (but not intermediate) values are predictive of churn.  In order to accommodate this in an algorithm like linear regression, we'd need to generate polynomial (or bucketed) terms.  Instead, let's attempt to model this problem using gradient boosted trees.  Amazon SageMaker provides an XGBoost container that we can use to train in a managed, distributed setting, and then host as a real-time prediction endpoint.  XGBoost uses gradient boosted trees which naturally account for non-linear relationships between features and the target variable, as well as accommodating complex interactions between features.\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a CSV or LibSVM format.  For this example, we'll stick with CSV.  It should:\n",
    "- Have the predictor variable in the first column\n",
    "- Not have a header row\n",
    "\n",
    "But first, let's convert our categorical features into numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完成最终数据集的组装\n",
    "model_data = pd.get_dummies(churn)\n",
    "model_data = pd.concat([model_data['Churn?_True.'], model_data.drop(['Churn?_False.', 'Churn?_True.'], axis=1)], axis=1)\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's split the data into training, validation, and test sets.  This will help prevent us from overfitting the model, and allow us to test the models accuracy on data it hasn't already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分3套数据集 训练、验证、测试\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload these files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train 模型训练\n",
    "\n",
    "Moving onto training, first we'll need to specify the locations of the XGBoost algorithm containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取XGBoost镜像\n",
    "container = sagemaker.image_uris.retrieve('xgboost', boto3.Session().region_name, '1.0-1')\n",
    "display(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `TrainingInput`s that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = TrainingInput(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "\n",
    "More detail on XGBoost's hyperparmeters can be found on their GitHub [page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md).\n",
    "- eta [default=0.3] 为了防止过拟合，更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获得新特征的权重。 eta通过缩减特征的权重使提升计算过程更加保守。缺省值为0.3 取值范围为：[0,1]\n",
    "- gamma [default=0] minimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be. 取值范围为：[0,∞]\n",
    "- max_depth [default=6] 树的最大深度。缺省值为6 取值范围为：[1,∞]\n",
    "- min_child_weight [default=1] 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。在现行回归模型中，这个参数是指建立每个模型所需要的最小样本数。该成熟越大算法越conservative 取值范围为：[0,∞]\n",
    "- max_delta_step [default=0] 我们允许每个树的权重被估计的值。如果它的值被设置为0，意味着没有约束；如果它被设置为一个正值，它能够使得更新的步骤更加保守。通常这个参数是没有必要的，但是如果在逻辑回归中类极其不平衡这时候他有可能会起到帮助作用。把它范围设置为1-10之间也许能控制更新。 取值范围为：[0,∞]\n",
    "- subsample [default=1] 用于训练模型的子样本占整个样本集合的比例。如果设置为0.5则意味着XGBoost将随机的从整个样本集合中随机的抽取出50%的子样本建立树模型，这能够防止过拟合。 取值范围为：(0,1]\n",
    "- colsample_bytree [default=1] 在建立树时对特征采样的比例。缺省值为1 取值范围为：(0,1]\n",
    "- colsample_bylevel [default=1]：树的每个层级分裂时子样本的特征所占的比例。作者表示不用这个参数，因为subsample和colsample_bytree组合做的事与之类似\n",
    "- lambda [default=1]： l2正则化权重的术语\n",
    "- alpha [default=0] ：l1正则化的权重术语。当特征量特别多的时候可以使用，这样能加快算法的运行效率\n",
    "- n_estimators 最佳迭代次数，树的个数\n",
    "\n",
    "AUC值：考虑样本不均衡情况，大部分采用AUC值进行模型评估，值越大代表效果越好¶\n",
    "- AUC的概率意义是随机取一对正负样本，正样本得分大于负样本的概率\n",
    "- AUC的最小值为0.5，最大值为1，取值越高越好\n",
    "- AUC=1，完美分类器，采用这个预测模型时，不管设定什么阀值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。\n",
    "- 0.5<AU<1，优于随机猜测。这个分类器(模型)妥善设定阀值的话，能有预测价值\n",
    "- 最终AUC的范围在[0.5，1]之间，并且越接近1越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入Debugger库\n",
    "from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig, CollectionConfig\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "create_time = strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "save_interval = 5\n",
    "\n",
    "# 创建一个实验，方便多个实验的指标进行对比\n",
    "customer_churn_experiment = Experiment.create(experiment_name=\"demo-customer-churn-experiment-{}\".format(create_time),\n",
    "                                              description=\"Using xgboost to predict customer churn\",\n",
    "                                              sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "# 设置超参数\n",
    "hyperparas = {\"max_depth\":5, # 构建树的深度，越大越容易过拟合\n",
    "              \"eta\":0.2, # 如同学习率\n",
    "              \"gamma\":4, # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2\n",
    "              \"min_child_weight\":9,\n",
    "              \"subsample\":0.8, # 随机采样训练样本\n",
    "              \"silent\":0, # 设置成1则没有运行信息输出，最好是设置为0\n",
    "              \"objective\":'binary:logistic',\n",
    "              \"eval_metric\":'auc', \n",
    "              \"num_round\":6} #训练轮数\n",
    "\n",
    "trial = Trial.create(trial_name=\"demo-trial-{}-weight-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()), hyperparas[\"min_child_weight\"]),\n",
    "                    experiment_name=customer_churn_experiment.experiment_name,\n",
    "                    sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "job_name = \"demo-customer-churn-job\"\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    hyperparameters=hyperparas,\n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    # 使用Spot实例进行训练\n",
    "                                    use_spot_instances=True,\n",
    "                                    max_run=3600,\n",
    "                                    max_wait=7200,\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess,\n",
    "                                    # debugger hook设置\n",
    "                                    debugger_hook_config=DebuggerHookConfig(\n",
    "                                        s3_output_path='s3://{}/{}/debugger'.format(bucket, prefix),  # Required\n",
    "                                        collection_configs=[\n",
    "                                            CollectionConfig(\n",
    "                                                name=\"metrics\",\n",
    "                                                parameters={\n",
    "                                                    \"save_interval\": str(save_interval)\n",
    "                                                }\n",
    "                                            ),\n",
    "                                            CollectionConfig(\n",
    "                                                name=\"feature_importance\",\n",
    "                                                parameters={\n",
    "                                                    \"save_interval\": str(save_interval)\n",
    "                                                }\n",
    "                                            ),\n",
    "                                            CollectionConfig(\n",
    "                                                name=\"full_shap\",\n",
    "                                                parameters={\n",
    "                                                    \"save_interval\": str(save_interval)\n",
    "                                                }\n",
    "                                            ),\n",
    "                                            CollectionConfig(\n",
    "                                                name=\"average_shap\",\n",
    "                                                parameters={\n",
    "                                                    \"save_interval\": str(save_interval)\n",
    "                                                }\n",
    "                                            ),\n",
    "                                        ],\n",
    "                                    ),\n",
    "\n",
    "                                    # 设置 Debugger的规则\n",
    "                                    rules=[\n",
    "                                        # 检测损失值是否以适当的速度减少\n",
    "                                        Rule.sagemaker(\n",
    "                                            rule_configs.loss_not_decreasing(),\n",
    "                                            rule_parameters={\n",
    "                                                \"collection_names\": \"metrics\",\n",
    "                                                \"num_steps\": str(save_interval * 2),\n",
    "                                            },\n",
    "                                        ),\n",
    "                                        # 检测是否过度训练\n",
    "                                        Rule.sagemaker(rule_configs.overtraining()),                                        \n",
    "                                        # 检测模型是否与测试数据过度拟合\n",
    "                                        Rule.sagemaker(rule_configs.overfit())\n",
    "                                    ],)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation},\n",
    "        # 实验的配置信息\n",
    "       experiment_config={\"ExperimentName\":customer_churn_experiment.experiment_name,\n",
    "                         \"TrialName\":trial.trial_name,\n",
    "                         \"TrialComponentDisplayName\":\"Training\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for _ in range(36):\n",
    "    job_name = xgb.latest_training_job.name\n",
    "    client = xgb.sagemaker_session.sagemaker_client\n",
    "    description = client.describe_training_job(TrainingJobName=job_name)\n",
    "    training_job_status = description[\"TrainingJobStatus\"]\n",
    "    rule_job_summary = xgb.latest_training_job.rule_job_summary()\n",
    "    rule_evaluation_status = rule_job_summary[0][\"RuleEvaluationStatus\"]\n",
    "    print(\"Training job status: {}, Rule Evaluation Status: {}\".format(training_job_status, rule_evaluation_status))\n",
    "    \n",
    "    if training_job_status in [\"Completed\", \"Failed\"]:\n",
    "        break\n",
    "\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.latest_training_job.rule_job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from smdebug.trials import create_trial\n",
    "\n",
    "# s3_output_path = xgb.latest_job_debugger_artifacts_path()\n",
    "# trial = create_trial(s3_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial.tensor(\"average_shap/f1\").values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import re\n",
    "\n",
    "\n",
    "# def get_data(trial, tname):\n",
    "#     \"\"\"\n",
    "#     For the given tensor name, walks though all the iterations\n",
    "#     for which you have data and fetches the values.\n",
    "#     Returns the set of steps and the values.\n",
    "#     \"\"\"\n",
    "#     tensor = trial.tensor(tname)\n",
    "#     steps = tensor.steps()\n",
    "#     vals = [tensor.value(s) for s in steps]\n",
    "#     return steps, vals\n",
    "\n",
    "# def plot_collection(trial, collection_name, regex='.*', figsize=(8, 6)):\n",
    "#     \"\"\"\n",
    "#     Takes a `trial` and a collection name, and \n",
    "#     plots all tensors that match the given regex.\n",
    "#     \"\"\"\n",
    "#     fig, ax = plt.subplots(figsize=figsize)\n",
    "#     sns.despine()\n",
    "\n",
    "#     tensors = trial.collection(collection_name).tensor_names\n",
    "\n",
    "#     for tensor_name in sorted(tensors):\n",
    "#         if re.match(regex, tensor_name):\n",
    "#             steps, data = get_data(trial, tensor_name)\n",
    "#             ax.plot(steps, data, label=tensor_name)\n",
    "\n",
    "#     ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#     ax.set_xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_collection(trial, \"metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_feature_importance(trial, importance_type=\"weight\"):\n",
    "#     SUPPORTED_IMPORTANCE_TYPES = [\"weight\", \"gain\", \"cover\", \"total_gain\", \"total_cover\"]\n",
    "#     if importance_type not in SUPPORTED_IMPORTANCE_TYPES:\n",
    "#         raise ValueError(f\"{importance_type} is not one of the supported importance types.\")\n",
    "#     plot_collection(\n",
    "#         trial,\n",
    "#         \"feature_importance\",\n",
    "#         regex=f\"feature_importance/{importance_type}/.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_feature_importance(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_feature_importance(trial, importance_type=\"cover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_collection(trial,\"average_shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Compile 编译模型\n",
    "[Amazon SageMaker Neo](https://aws.amazon.com/sagemaker/neo/) optimizes models to run up to twice as fast, with no loss in accuracy. When calling `compile_model()` function, we specify the target instance family (m4) as well as the S3 bucket to which the compiled model would be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = xgb\n",
    "output_path = '/'.join(xgb.output_path.split('/')[:-1])\n",
    "# NEO 编译模型，优化性能\n",
    "compiled_model = xgb.compile_model(target_instance_family='ml_m4', \n",
    "                                   input_shape={'data': [1, 69]},\n",
    "                                   role=role,\n",
    "                                   framework='xgboost',\n",
    "                                   framework_version='latest',\n",
    "                                   output_path=output_path)\n",
    "compiled_model.name = 'deployed-xgboost-customer-churn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Host 托管终端节点\n",
    "\n",
    "Now that we've trained the algorithm, let's create a model and deploy it to a hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "# 配置模型监控\n",
    "data_capture_prefix = f'{prefix}/datacapture'\n",
    "s3_capture_upload_path = f's3://{bucket}/{data_capture_prefix}'\n",
    "\n",
    "endpoint_name = f\"xgb-customer-churn-model-quality-monitor-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "print(\"EndpointName =\", endpoint_name)\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "                        enable_capture=True,\n",
    "                        sampling_percentage=100,\n",
    "                        destination_s3_uri=s3_capture_upload_path)\n",
    "\n",
    "# 部署模型，提供EndPoint支持推理API\n",
    "xgb_predictor = compiled_model.deploy(\n",
    "    initial_instance_count = 1, \n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=CSVSerializer(),\n",
    "    data_capture_config=data_capture_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s3_capture_upload_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate 评估模型\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, simply by making an http POST request.  But first, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batchs to CSV string payloads\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "# 基于测试数据，准备预测验证数据\n",
    "predictions = predict(test_data.to_numpy()[:,1:])\n",
    "\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values.  In this case, we're simply predicting whether the customer churned (`1`) or not (`0`), which produces a simple confusion matrix. 混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note, due to randomized elements of the algorithm, you results may differ slightly._\n",
    "\n",
    "Of the 48 churners, we've correctly predicted 39 of them (true positives). And, we incorrectly predicted 4 customers would churn who then ended up not doing so (false positives).  There are also 9 customers who ended up churning, that we predicted would not (false negatives).\n",
    "\n",
    "An important point here is that because of the `np.round()` function above we are using a simple threshold (or cutoff) of 0.5.  Our predictions from `xgboost` come out as continuous values between 0 and 1 and we force them into the binary classes that we began with.  However, because a customer that churns is expected to cost the company more than proactively trying to retain a customer who we think might churn, we should consider adjusting this cutoff.  That will almost certainly increase the number of false positives, but it can also be expected to increase the number of true positives and reduce the number of false negatives.\n",
    "\n",
    "To get a rough intuition here, let's look at the continuous values of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous valued predictions coming from our model tend to skew toward 0 or 1, but there is sufficient mass between 0.1 and 0.9 that adjusting the cutoff should indeed shift a number of customers' predictions.  For example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.3, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that changing the cutoff from 0.5 to 0.3 results in 1 more true positives, 3 more false positives, and 1 fewer false negatives.  The numbers are small overall here, but that's 6-10% of customers overall that are shifting because of a change to the cutoff.  Was this the right decision?  We may end up retaining 3 extra customers, but we also unnecessarily incentivized 5 more customers who would have stayed.  Determining optimal cutoffs is a key step in properly applying machine learning in a real-world setting.  Let's discuss this more broadly and then apply a specific, hypothetical solution for our current problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extensions 扩展可选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic model Tuning (optional) 自动模型调优\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose.\n",
    "For example, suppose that you want to solve a binary classification problem on this marketing dataset. Your goal is to maximize the area under the curve (auc) metric of the algorithm by training an XGBoost Algorithm model. You don't know which values of the eta, alpha, min_child_weight, and max_depth hyperparameters to use to train the best model. To find the best values for these hyperparameters, you can specify ranges of values that Amazon SageMaker hyperparameter tuning searches to find the combination of values that results in the training job that performs the best as measured by the objective metric that you chose. Hyperparameter tuning launches training jobs that use hyperparameter values in the ranges that you specified, and returns the training job with highest auc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "# 设定超参的取值范围\n",
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                            'min_child_weight': ContinuousParameter(1, 10),\n",
    "                            'alpha': ContinuousParameter(0, 2),\n",
    "                            'max_depth': IntegerParameter(1, 10)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_metric 评价指标可选范围\n",
    "\n",
    "- “rmse”: root mean square error\n",
    "- “logloss”: negative log-likelihood\n",
    "- “error”: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.\n",
    "- “merror”: Multiclass classification error rate. It is calculated as #(wrongcases)#(allcases).\n",
    "- “mlogloss”: Multiclass logloss\n",
    "- “auc”: Area under the curve for ranking evaluation.\n",
    "- “ndcg”:Normalized Discounted Cumulative Gain\n",
    "- “map”:Mean average precision\n",
    "- “ndcg@n”,”map@n”: n can be assigned as an integer to cut off the top positions in the lists for evaluation.\n",
    "- “ndcg-“,”map-“,”ndcg@n-“,”map@n-“: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions. training repeatively\n",
    "\n",
    "### AUC值：考虑样本不均衡情况，大部分采用AUC值进行模型评估，值越大代表效果越好\n",
    "\n",
    "- AUC的概率意义是随机取一对正负样本，正样本得分大于负样本的概率\n",
    "- AUC的最小值为0.5，最大值为1，取值越高越好\n",
    "- AUC=1，完美分类器，采用这个预测模型时，不管设定什么阀值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。\n",
    "- 0.5<AU<1，优于随机猜测。这个分类器(模型)妥善设定阀值的话，能有预测价值\n",
    "- 最终AUC的范围在[0.5，1]之间，并且越接近1越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估指标选择 auc\n",
    "objective_metric_name = 'validation:auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=9,\n",
    "                            max_parallel_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the best training job name\n",
    "tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Deploy the best trained or user specified model to an Amazon SageMaker endpoint\n",
    "tuner_predictor = tuner.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a serializer\n",
    "tuner_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions and convert from the CSV output our model provides into a NumPy array\n",
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.3, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Clean-up\n",
    "\n",
    "If you're ready to be done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
